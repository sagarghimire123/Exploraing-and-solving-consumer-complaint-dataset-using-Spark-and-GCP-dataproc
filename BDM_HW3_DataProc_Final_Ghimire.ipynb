{"cells":[{"cell_type":"markdown","metadata":{"id":"gbF71GHptuwV"},"source":["# Homework 3\n","\n","We are greatly inspired by the [Consumer Complaints](https://github.com/InsightDataScience/consumer_complaints) challenge from [InsightDataScience](https://github.com/InsightDataScience/). In fact, we are going to tackle the same challenge but using Apache Spark. Please read through the challenge at the following link:\n","\n","<https://github.com/InsightDataScience/consumer_complaints>\n","\n","The most important sections are **Input dataset** and **Expected output**, which are quoted below:\n","\n","## Input dataset\n","For this challenge, when we grade your submission, an input file, `complaints.csv`, will be moved to the top-most `input` directory of your repository. Your code must read that input file, process it and write the results to an output file, `report.csv` that your code must place in the top-most `output` directory of your repository.\n","\n","Below are the contents of an example `complaints.csv` file:\n","```\n","Date received,Product,Sub-product,Issue,Sub-issue,Consumer complaint narrative,Company public response,Company,State,ZIP code,Tags,Consumer consent provided?,Submitted via,Date sent to company,Company response to consumer,Timely response?,Consumer disputed?,Complaint ID\n","2019-09-24,Debt collection,I do not know,Attempts to collect debt not owed,Debt is not yours,\"transworld systems inc. is trying to collect a debt that is not mine, not owed and is inaccurate.\",,TRANSWORLD SYSTEMS INC,FL,335XX,,Consent provided,Web,2019-09-24,Closed with explanation,Yes,N/A,3384392\n","2019-09-19,\"Credit reporting, credit repair services, or other personal consumer reports\",Credit reporting,Incorrect information on your report,Information belongs to someone else,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,Experian Information Solutions Inc.,PA,15206,,Consent not provided,Web,2019-09-20,Closed with non-monetary relief,Yes,N/A,3379500\n","2020-01-06,\"Credit reporting, credit repair services, or other personal consumer reports\",Credit reporting,Incorrect information on your report,Information belongs to someone else,,,Experian Information Solutions Inc.,CA,92532,,N/A,Email,2020-01-06,In progress,Yes,N/A,3486776\n","2019-10-24,\"Credit reporting, credit repair services, or other personal consumer reports\",Credit reporting,Incorrect information on your report,Information belongs to someone else,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,\"TRANSUNION INTERMEDIATE HOLDINGS, INC.\",CA,925XX,,Other,Web,2019-10-24,Closed with explanation,Yes,N/A,3416481\n","2019-11-20,\"Credit reporting, credit repair services, or other personal consumer reports\",Credit reporting,Incorrect information on your report,Account information incorrect,I would like the credit bureau to correct my XXXX XXXX XXXX XXXX balance. My correct balance is XXXX,Company has responded to the consumer and the CFPB and chooses not to provide a public response,\"TRANSUNION INTERMEDIATE HOLDINGS, INC.\",TX,77004,,Consent provided,Web,2019-11-20,Closed with explanation,Yes,N/A,3444592\n","```\n","Each line of the input file, except for the first-line header, represents one complaint. Consult the [Consumer Finance Protection Bureau's technical documentation](https://cfpb.github.io/api/ccdb/fields.html) for a description of each field.  \n","\n","* Notice that complaints were not listed in chronological order\n","* In 2019, there was a complaint against `TRANSWORLD SYSTEMS INC` for `Debt collection`\n","* Also in 2019, `Experian Information Solutions Inc.` received one complaint for `Credit reporting, credit repair services, or other personal consumer reports` while `TRANSUNION INTERMEDIATE HOLDINGS, INC.` received two\n","* In 2020, `Experian Information Solutions Inc.` received a complaint for `Credit reporting, credit repair services, or other personal consumer reports`\n","\n","In summary that means\n","* In 2019, there was one complaint for `Debt collection`, and 100% of it went to one company\n","* Also in 2019, three complaints against two companies were received for `Credit reporting, credit repair services, or other personal consumer reports` and 2/3rd of them (or 67% if we rounded the percentage to the nearest whole number) were against one company (TRANSUNION INTERMEDIATE HOLDINGS, INC.)\n","* In 2020, only one complaint was received for `Credit reporting, credit repair services, or other personal consumer reports`, and so the highest percentage received by one company would be 100%\n","\n","For this challenge, we want for each product and year that complaints were received, the total number of complaints, number of companies receiving a complaint and the highest percentage of complaints directed at a single company.\n","\n","For the purposes of this challenge, all names, including company and product, should be treated as case insensitive. For example, \"Acme\", \"ACME\", and \"acme\" would represent the same company.\n","\n","## Expected output\n","\n","After reading and processing the input file, your code should create an output file, `report.csv`, with as many lines as unique pairs of product and year (of `Date received`) in the input file.\n","\n","Each line in the output file should list the following fields in the following order:\n","* product (name should be written in all lowercase)\n","* year\n","* total number of complaints received for that product and year\n","* total number of companies receiving at least one complaint for that product and year\n","* highest percentage (rounded to the nearest whole number) of total complaints filed against one company for that product and year. Use standard rounding conventions (i.e., Any percentage between 0.5% and 1%, inclusive, should round to 1% and anything less than 0.5% should round to 0%)\n","\n","The lines in the output file should be sorted by product (alphabetically) and year (ascending)\n","\n","Given the above `complaints.csv` input file, we'd expect an output file, `report.csv`, in the following format\n","```\n","\"credit reporting, credit repair services, or other personal consumer reports\",2019,3,2,67\n","\"credit reporting, credit repair services, or other personal consumer reports\",2020,1,1,100\n","debt collection,2019,1,1,100\n","```\n","Notice that because `debt collection` was only listed for 2019 and not 2020, the output file only has a single entry for debt collection. Also, notice that when a product has a comma (`,`) in the name, the name should be enclosed by double quotation marks (`\"`). Finally, notice that percentages are listed as numbers and do not have `%` in them.\n","\n","# Objectives\n","\n","In this homework, we will tackle the above problem in two steps (2 tasks):\n","\n","1. In Task 1, we work on a solution with PySpark on Google Colab using a sample of the data. The data is available on Google Drive and is to be downloaded by the `gdown` command in Task 1.\n","\n","2. In Task 2, we create a standalone Python script that work on the full dataset using GCP DataProc. The full dataset is downloaded from [here](https://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data). The data is available on the class bucket as: `gs://bdma/data/complaints.csv`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yI2LPTUcu0Hx"},"source":[]},{"cell_type":"markdown","metadata":{"id":"VBWF5LNefozN"},"source":["## Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjzjcPWYnHLr","outputId":"6c8bcd7b-7cf4-46a2-beb4-8f12d3f9d812"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"data":{"text/plain":[]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","gdown --quiet 1-IeoZDwT5wQzBUpsaS5B6vTaP-2ZBkam\n","pip --quiet install pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"XHKq10WXnZl7","outputId":"bc02b6cf-0375-4b3b-9909-d723858e599a"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://66e91eaec4ce:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fca42f6c790>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["COMPLAINTS_FN = 'complaints_sample.csv'\n","\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","sc = pyspark.SparkContext.getOrCreate()\n","spark = SparkSession.builder.getOrCreate()\n","spark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PATb9nVZUIzR","outputId":"ae4e6bbe-c585-4125-df24-aab5e8d1eca4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Date received,Product,Sub-product,Issue,Sub-issue,Consumer complaint narrative,Company public response,Company,State,ZIP code,Tags,Consumer consent provided?,Submitted via,Date sent to company,Company response to consumer,Timely response?,Consumer disputed?,Complaint ID\r\n","2015-12-31,Bank account or service,Checking account,\"Making/receiving payments, sending money\",,,,FIRSTBANK PUERTO RICO,PR,00902,Older American,N/A,Referral,2016-02-04,Closed with explanation,Yes,No,1723943\r\n","2016-03-15,Bank account or service,Other bank product/service,Problems caused by my funds being low,,,,FIRSTBANK PUERTO RICO,PR,00926,,Consent not provided,Web,2016-03-15,Closed with explanation,Yes,No,1833740\r\n","2016-10-24,Bank account or service,Checking account,\"Account opening, closing, or management\",,\"In the month of XX/XX/2015, my email address ( XXXX ) was hacked and used to send messages to people associated with my business. At that time, transactions for the purchase and sales of products were made. The hacker forged the identities of our customers and suppliers, creating email addresses similar to those recorded in our email account, which they then used to communicate with our customers, our suppliers, and us. \n","\n","The hackers fraudulently and inexplicably opened XXXX ( XXXX ) accounts with WELLS FARGO BANK in XXXX XXXX, California.\",Company has responded to the consumer and the CFPB and chooses not to provide a public response,WELLS FARGO & COMPANY,PR,00969,,Consent provided,Web,2016-12-28,Closed with non-monetary relief,Yes,No,2175792\r\n","2017-09-08,Checking or savings account,Checking account,Managing an account,Deposits and withdrawals,,,Comerica,TX,77551,,N/A,Referral,2017-11-07,Closed with explanation,Yes,N/A,2668920\r\n","2018-09-19,Checking or savings account,Checking account,Problem with a lender or other company charging your account,Money was taken from your account on the wrong day or for the wrong amount,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,WELLS FARGO & COMPANY,FL,33326,,Consent not provided,Web,2018-09-19,Closed with explanation,Yes,N/A,3023007\r\n","2018-12-04,Checking or savings account,Checking account,Managing an account,Deposits and withdrawals,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,\"BANK OF AMERICA, NATIONAL ASSOCIATION\",MI,48146,,N/A,Referral,2018-12-11,Closed with explanation,Yes,N/A,3093436\r\n","2018-12-05,Checking or savings account,Other banking product or service,Managing an account,Deposits and withdrawals,\"On the XX/XX/2018, I was unable to access funds in my account using my debit card at a car dealership, despite adequate funds being available in my account. The amount was {$12000.00}. \n","\n","Despite contacting the bank via telephone and spending over an hour on the phone with them ( being passed from department to department ), the bank refused to authorise my payment. After many different attempts at rectifying the issue by several different departments, the bank finally told me that a {$5000.00} daily transaction limit ( that I had previously neither agreed upon or knew about ), could not be waived even with my authorisation. The call centre also gave poor instruction to the dealership on how to authorise the transaction as debit, which led to a {$3000.00} transaction going through as credit ( giving the dealership a financial penalty which luckily they accepted on my behalf ). The bank then tried to force me to do a wire transfer, and didn't tell me there would be a charge until I was just about to authorise the transaction. Luckily I did not complete the transaction. \n","\n","I was forced to leave the dealership and complete a 2 hour round trip to retrieve a check book from home. \n","\n","The bank is HSBC US, though all of the departments I spoke to on the phone were overseas call centres.\",,HSBC NORTH AMERICA HOLDINGS INC.,FL,320XX,Servicemember,Consent provided,Web,2018-12-05,Closed with explanation,Yes,N/A,3092091\r\n","2018-12-06,Checking or savings account,Checking account,Problem with a lender or other company charging your account,Money was taken from your account on the wrong day or for the wrong amount,\"Check # XXXX was paid into my account on XX/XX/18 for the amount of XXXX.\n","\n","The account holder of the check and I bank with the same bank chase. On the XX/XX/18 the funds were place on hold and when I call about the funds I was told they needed to spoke with the person who wrote the check. After speaking with that person who gave me the check the funds were released on the XX/XX/18. On the XX/XX/18 the funds were put back on hold again, and when I call the bank, they sent me to the recovery and Fraud department and my account was overdraw. I later got the check in the mail and the letter stated my couldnt verfity the hand writing. My account is overdraw still and I want my monies.\",,JPMORGAN CHASE & CO.,TX,770XX,,Consent provided,Web,2018-12-06,Closed with explanation,Yes,N/A,3093052\r\n","2018-12-10,Checking or savings account,Checking account,Managing an account,Problem accessing account,I added my son to my NFCU business account as an authorized user about 2 years ago or so. He was added with no problem. Today XX/XX/2018 my business assets are frozen because he owes them money and they blocked me out of my own business accounts. Apparently he owes them over {$5000.00} and they restricted my account because according ti them they can do that. My son went into a branch as was grossly discriminated against and treated like a criminal when I sent him to get a cashiers check. He has called in several times with me and they have been rude and We have been hung up on and when trying to speak to a supervisor I am advised that there is no one working that someone will return my call in 24 to 48 hours. According to what I know these accounts are from XX/XX/2010 and they are declining to forward information to validate the debt. On the recorded line the rep insisted he say that He acknowledge that he owes the money after repeated attempts by the rep in their recovery she hung up and became rude. By them restricting access to my buainess account it will cost me a total of about XXXX to XXXX in fees and other legal costs because of that. My business should not be responsible of them not verifying his information when I added him over 2 years ago. They should have said no of instead taking opportunity to try to collect on a business account that frankly he is only authorized to go when I am not available. I feel that these actions were initiated as a retaliation because they made me go through hoops about a code word on the account and because I stated that I would report them to the proper authorities.,Company believes it acted appropriately as authorized by contract or law,NAVY FEDERAL CREDIT UNION,AZ,853XX,,Consent provided,Web,2018-12-10,Closed with explanation,Yes,N/A,3096884\r\n"]}],"source":["!head -n 20 {COMPLAINTS_FN}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zM50tNXNWV6R"},"outputs":[],"source":["## Importing all necessary libraries for calculation\n","from pyspark.sql.functions import year\n","from pyspark.sql.functions import lower, year, count, round\n","from pyspark.sql.functions import sum, expr\n","from pyspark.sql.types import StringType\n","from pyspark.sql.types import IntegerType\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import countDistinct\n","from pyspark.sql.functions import col, count, sum, ceil, max\n","from pyspark.sql.window import Window"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oa_06D8TUX7q","outputId":"0d219af1-5387-42a9-be63-75d0ee3550fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+--------------------+------------+\n","|             Product|year|             Company|Complaint ID|\n","+--------------------+----+--------------------+------------+\n","|bank account or s...|2015|firstbank puerto ...|     1723943|\n","|bank account or s...|2016|firstbank puerto ...|     1833740|\n","+--------------------+----+--------------------+------------+\n","only showing top 2 rows\n","\n"]}],"source":["## Reading the csv file \"COMPLAINTS_FN\"\n","df=spark.read.csv(COMPLAINTS_FN,header=True, multiLine=True, escape='\"')\n","## selecting only 4 columns for the analysis which are Date received(take only year), Product, Company and Complaint ID\n","dyear=df.select(\"Product\",(year(\"Date received\")).alias(\"year\"),\"Company\",\"Complaint ID\").sort(\"Product\",\"year\")\n","## Lowercasing the Product and Company columns values for further grouping and other calculation\n","ds_product = dyear.withColumn(\"Product\", lower(dyear[\"Product\"].cast(StringType())))\n","ds_company = ds_product.withColumn(\"Company\", lower(ds_product[\"Company\"].cast(StringType())))\n","## Print the dataframe after the cleaning process and named as 'ds_company'\n","ds_company.show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-u4-PcEU2h1","outputId":"6b6a843a-43b1-408f-a6d6-e0ce06a51143"},"outputs":[{"data":{"text/plain":["6623"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["## Print the count of the data frame to check number of rows\n","ds_company.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8n_ylSIabJ-8","outputId":"eac24725-0481-40b9-efea-dabe5c96cdaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+----------------+---------------+\n","|             Product|year|total_complaints|Companies Count|\n","+--------------------+----+----------------+---------------+\n","|bank account or s...|2015|               1|              1|\n","|bank account or s...|2016|               2|              2|\n","+--------------------+----+----------------+---------------+\n","only showing top 2 rows\n","\n"]}],"source":["\n","##Grouping the 'ds_company' dataframe  by product and year\n","##counting the number of Compplaint ID  and number of distinct Company for total complaints and Companies count\n","ds_complaint=ds_company.groupBy(\"Product\", \"year\").agg(countDistinct(\"Complaint ID\").alias(\"total_complaints\"),countDistinct(\"Company\").alias(\"Companies Count\")).sort(\"Product\",\"year\")\n","ds_complaint.show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CspOWPv-OKz5","outputId":"961e5227-f405-4faf-bb40-eaa5f41bdb32"},"outputs":[{"data":{"text/plain":["46"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["## Copunt of the grouped dsataframe \"ds_complaint\"\n","ds_complaint.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naRQWraQ9NjK","outputId":"feacb63b-efca-4b9e-89e6-4460f34fc5b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+--------------------+-----------------------+\n","|             Product|year|             Company|atleast_complaint_count|\n","+--------------------+----+--------------------+-----------------------+\n","|credit reporting,...|2019|service finance h...|                      1|\n","|     debt collection|2019|security credit s...|                      1|\n","+--------------------+----+--------------------+-----------------------+\n","only showing top 2 rows\n","\n"]}],"source":["# Group by product, year, and company, and count Complaint ID for atleast one  complain of the company\n","## The grouped new data frame is \"grouped_df\"\n","grouped_df = ds_company.groupBy(\"Product\", \"year\", \"Company\").agg(count(\"Complaint ID\").alias(\"atleast_complaint_count\"))\n","grouped_df.show(2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lnStMWrONzZ","outputId":"b23d7e30-f93c-4213-96d4-dc3772a12afa"},"outputs":[{"data":{"text/plain":["1209"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["##\"grouped_df\" count for rows\n","grouped_df.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GuG18qAMwZj","outputId":"6a32ee8b-8d3d-40c0-9fcd-796ea8bbffa1"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+----------------+---------------+--------------------+-----------------------+\n","|             Product|year|total_complaints|Companies Count|             Company|atleast_complaint_count|\n","+--------------------+----+----------------+---------------+--------------------+-----------------------+\n","|credit reporting,...|2019|            3114|            203|phoenix financial...|                      1|\n","|credit reporting,...|2019|            3114|            203|        synovus bank|                      2|\n","+--------------------+----+----------------+---------------+--------------------+-----------------------+\n","only showing top 2 rows\n","\n"]}],"source":["## Joining two data frames by Product and year\n","grouped_df1=ds_complaint.join(grouped_df, [\"Product\", \"year\"])\n","grouped_df1.show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"31s3_2hPOCVC","outputId":"bce4a830-d775-4730-d865-26dc055f8eb1"},"outputs":[{"data":{"text/plain":["1209"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["## Rows count of \"grouped_df1\"\n","grouped_df1.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vKq2xvC9-SMD","outputId":"e39242f4-09a6-4efd-ce60-113cb40cfcd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+----------------+---------------+--------------------+-----------------------+----------+\n","|             Product|year|total_complaints|Companies Count|             Company|atleast_complaint_count|percentage|\n","+--------------------+----+----------------+---------------+--------------------+-----------------------+----------+\n","|bank account or s...|2015|               1|              1|firstbank puerto ...|                      1|       100|\n","|bank account or s...|2016|               2|              2|wells fargo & com...|                      1|        50|\n","+--------------------+----+----------------+---------------+--------------------+-----------------------+----------+\n","only showing top 2 rows\n","\n"]}],"source":["# Calculate the percentage of total complaints filed against one company for each product and year by dividing with 'total_complaints'\n","grouped_df2 = grouped_df1.withColumn((\"percentage\"), round(grouped_df1[\"atleast_complaint_count\"] / grouped_df1[\"total_complaints\"] * 100, 0).cast(IntegerType())).sort(\"Product\",\"year\")\n","grouped_df2.show(2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D4dAXlT4A_Vj","outputId":"be6e7c69-3f37-4dda-d162-a5967ed864ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+----------------+---------------+--------------------+-----------------------+----------+\n","|             Product|year|total_complaints|Companies Count|             Company|atleast_complaint_count|percentage|\n","+--------------------+----+----------------+---------------+--------------------+-----------------------+----------+\n","|bank account or s...|2015|               1|              1|firstbank puerto ...|                      1|       100|\n","|bank account or s...|2016|               2|              2|wells fargo & com...|                      1|        50|\n","+--------------------+----+----------------+---------------+--------------------+-----------------------+----------+\n","only showing top 2 rows\n","\n"]}],"source":["## Joining the 'ds_complaint' dataframe and \"grouped_df2\" dataframe to bring all the above\n","##calculation together like company count, total complaint, atleast_complaint_count and percentage\n","dp=ds_complaint.join(grouped_df2, [\"Product\", \"year\",\"total_complaints\", \"Companies Count\"], \"left\").sort(\"Product\",\"year\")\n","dp.show(2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7dLkqT_7FxlI","outputId":"de246080-fa21-4909-c8b5-bde09484c4c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+------------------+\n","|             Product|year|highest_percentage|\n","+--------------------+----+------------------+\n","|bank account or s...|2015|               100|\n","|bank account or s...|2016|                50|\n","+--------------------+----+------------------+\n","only showing top 2 rows\n","\n"]}],"source":["## Grouping the dataframe 'dp' by product and year and printing only highest percentage\n","## of total complaints filed against one company for that product and year\n","gr = dp.groupBy(\"Product\", \"year\") .agg(max(\"percentage\").alias(\"highest_percentage\")).sort(\"Product\", \"year\")\n","gr.show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAMMIwZoIlK2","outputId":"fa27fcc9-2b2f-420a-da02-d95bec442f78"},"outputs":[{"data":{"text/plain":["46"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["## rows count of dataframe 'gr'\n","gr.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HtEO9krHuzc","outputId":"5eeeb350-f10d-4b35-9796-000610cea514"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+----+----------------+---------------+------------------+\n","|             Product|year|total_complaints|Companies Count|highest_percentage|\n","+--------------------+----+----------------+---------------+------------------+\n","|bank account or s...|2015|               1|              1|               100|\n","|bank account or s...|2016|               2|              2|                50|\n","|checking or savin...|2017|               1|              1|               100|\n","|checking or savin...|2018|              20|             10|                25|\n","|checking or savin...|2019|             461|             72|                13|\n","|checking or savin...|2020|               3|              3|                33|\n","|       consumer loan|2015|               1|              1|               100|\n","|       consumer loan|2016|               1|              1|               100|\n","|       consumer loan|2017|               1|              1|               100|\n","|         credit card|2016|               4|              4|                25|\n","|         credit card|2017|               1|              1|               100|\n","|credit card or pr...|2017|               1|              1|               100|\n","|credit card or pr...|2018|              27|             12|                33|\n","|credit card or pr...|2019|             437|             42|                15|\n","|credit card or pr...|2020|              13|             10|                23|\n","|credit reporting,...|2017|               7|              5|                29|\n","|credit reporting,...|2018|             238|             22|                56|\n","|credit reporting,...|2019|            3114|            203|                50|\n","|credit reporting,...|2020|             144|             10|                51|\n","|     debt collection|2015|               4|              3|                50|\n","+--------------------+----+----------------+---------------+------------------+\n","only showing top 20 rows\n","\n"]}],"source":["## merging dataframe 'ds' and 'gr' to put 'total complaints', 'companies count' and 'highest percentage of complaint' together\n","final=ds_complaint.join(gr,[\"Product\", \"year\"]).sort(\"Product\",\"year\")\n","final.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RtO8Xk0NXhf"},"outputs":[],"source":["## writning the csv file for above dataframe \"final\"\n","final.write.csv(\"report.csv\")\n","## Converting the csv into spark rdd\n","rdd=spark.sparkContext.textFile(\"report.csv\")\n","## assigning the desired output name for rdd\n","outputTask1=rdd"]},{"cell_type":"markdown","metadata":{"id":"iLmGIkF7Ky39"},"source":[]},{"cell_type":"markdown","metadata":{"id":"-9Ykp1Qqnu5f"},"source":["## Task 1\n","\n","Use PySpark to derive the expected output. Your computation must be done entirely on Spark's transformation. The output MUST be in the CSV form, i.e. each output line is a complete comma separated string that can be fed into a CSV reader. It is okay if your output are divided into multiple parts (due to the nature of distributed computing of Spark)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1r07D6MEMOq","outputId":"0a180ad3-d6e7-4cc0-9566-3228973151da"},"outputs":[{"data":{"text/plain":["['bank account or service,2015,1,1,100',\n"," 'bank account or service,2016,2,2,50',\n"," 'checking or savings account,2017,1,1,100',\n"," 'checking or savings account,2018,20,10,25',\n"," 'checking or savings account,2019,461,72,13',\n"," 'checking or savings account,2020,3,3,33',\n"," 'consumer loan,2015,1,1,100',\n"," 'consumer loan,2016,1,1,100',\n"," 'consumer loan,2017,1,1,100',\n"," 'credit card,2016,4,4,25',\n"," 'credit card,2017,1,1,100',\n"," 'credit card or prepaid card,2017,1,1,100',\n"," 'credit card or prepaid card,2018,27,12,33',\n"," 'credit card or prepaid card,2019,437,42,15',\n"," 'credit card or prepaid card,2020,13,10,23',\n"," '\"credit reporting, credit repair services, or other personal consumer reports\",2017,7,5,29',\n"," '\"credit reporting, credit repair services, or other personal consumer reports\",2018,238,22,56',\n"," '\"credit reporting, credit repair services, or other personal consumer reports\",2019,3114,203,50',\n"," '\"credit reporting, credit repair services, or other personal consumer reports\",2020,144,10,51',\n"," 'debt collection,2015,4,3,50']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# outputTask1 is an output RDD, you can use DataFrame as well but each line\n","# still needs to be a string\n","outputTask1.take(20)"]},{"cell_type":"markdown","metadata":{"id":"oNXJMMLEEcHB"},"source":["## Task 2\n","\n","For this task, please convert what you have in Task 1 to a standalone file that can be run on any DataProc cluster. The input and output locations must be taken from the command line, e.g. using my cluster named `bdma`:\n","\n","```shell\n","gcloud dataproc jobs submit pyspark --cluster bdma BDM_HW3_EMPLID_LastName.py -- gs://bdma/data/complaints.csv gs://bdma/shared/2023_spring/HW3/EMPLID_LastName\n","```\n","\n","As part of the test, you must be able to run your code and output to the class shared folder, i.e.: `gs://bdma/shared/2023_spring/HW3/EMPLID_LastName`, **replacing `EMPLID` and `LastName` with your actual EMPL ID and Last Name**.\n","\n","Note that, if you run your code multiple times, make sure to only run your working version when output to the shared folder, or you must remove the existing output to run your code again.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJecoKVsqJcJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTRRikA1prjI"},"outputs":[],"source":["!pip install google-cloud-dataproc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPL0z-3P1NHN"},"outputs":[],"source":["!gcloud auth login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_AB__Q01nbC"},"outputs":[],"source":["!gcloud projects list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyMrdrGw2lnq"},"outputs":[],"source":["!gcloud config set project bdma-384100\n","!gcloud config set compute/region us-west1\n","!gcloud config set compute/zone us-west1-a\n","!gcloud config set dataproc/region us-west1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RfcxiYbI3ORh"},"outputs":[],"source":["!gcloud dataproc clusters create bdm-hw3 --enable-component-gateway --region us-west1 --zone us-west1-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 2.0-debian10 --project bdma-384100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L9IcyXmS4RBP","outputId":"85028e37-5d2f-4a89-fb00-7c72223b7de3"},"outputs":[{"name":"stdout","output_type":"stream","text":["clusterName: bdm-hw3\n","clusterUuid: 6479032e-cccd-4572-82a1-8a7c6a1ba9a8\n","config:\n","  configBucket: dataproc-staging-us-west1-473379980650-f1bjrajq\n","  endpointConfig:\n","    enableHttpPortAccess: true\n","    httpPorts:\n","      HDFS NameNode: https://cjvc4yql3rb2vjowuoqncgkfuy-dot-us-west1.dataproc.googleusercontent.com/hdfs/dfshealth.html\n","      HiveServer2 (bdm-hw3-m): https://cjvc4yql3rb2vjowuoqncgkfuy-dot-us-west1.dataproc.googleusercontent.com/hiveserver2ui/bdm-hw3-m?host=bdm-hw3-m\n","      MapReduce Job History: https://cjvc4yql3rb2vjowuoqncgkfuy-dot-us-west1.dataproc.googleusercontent.com/jobhistory/\n","      Spark History Server: https://cjvc4yql3rb2vjowuoqncgkfuy-dot-us-west1.dataproc.googleusercontent.com/sparkhistory/\n","      Tez: https://cjvc4yql3rb2vjowuoqncgkfuy-dot-us-west1.dataproc.googleusercontent.com/apphistory/tez-ui/\n","      YARN Application Timeline: https://cjvc4yql3rb2vjowuoqncgkfuy-dot-us-west1.dataproc.googleusercontent.com/apphistory/\n","      YARN ResourceManager: https://cjvc4yql3rb2vjowuoqncgkfuy-dot-us-west1.dataproc.googleusercontent.com/yarn/\n","  gceClusterConfig:\n","    internalIpOnly: false\n","    networkUri: https://www.googleapis.com/compute/v1/projects/bdma-384100/global/networks/default\n","    serviceAccountScopes:\n","    - https://www.googleapis.com/auth/bigquery\n","    - https://www.googleapis.com/auth/bigtable.admin.table\n","    - https://www.googleapis.com/auth/bigtable.data\n","    - https://www.googleapis.com/auth/cloud.useraccounts.readonly\n","    - https://www.googleapis.com/auth/devstorage.full_control\n","    - https://www.googleapis.com/auth/devstorage.read_write\n","    - https://www.googleapis.com/auth/logging.write\n","    zoneUri: https://www.googleapis.com/compute/v1/projects/bdma-384100/zones/us-west1-a\n","  masterConfig:\n","    diskConfig:\n","      bootDiskSizeGb: 500\n","      bootDiskType: pd-standard\n","    imageUri: https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-2-0-deb10-20230415-165100-rc01\n","    instanceNames:\n","    - bdm-hw3-m\n","    machineTypeUri: https://www.googleapis.com/compute/v1/projects/bdma-384100/zones/us-west1-a/machineTypes/n1-standard-4\n","    minCpuPlatform: AUTOMATIC\n","    numInstances: 1\n","    preemptibility: NON_PREEMPTIBLE\n","  softwareConfig:\n","    imageVersion: 2.0.62-debian10\n","    properties:\n","      capacity-scheduler:yarn.scheduler.capacity.root.default.ordering-policy: fair\n","      core:fs.gs.block.size: '134217728'\n","      core:fs.gs.metadata.cache.enable: 'false'\n","      core:hadoop.ssl.enabled.protocols: TLSv1,TLSv1.1,TLSv1.2\n","      distcp:mapreduce.map.java.opts: -Xmx768m\n","      distcp:mapreduce.map.memory.mb: '1024'\n","      distcp:mapreduce.reduce.java.opts: -Xmx768m\n","      distcp:mapreduce.reduce.memory.mb: '1024'\n","      hadoop-env:HADOOP_DATANODE_OPTS: -Xmx512m\n","      hdfs:dfs.datanode.address: 0.0.0.0:9866\n","      hdfs:dfs.datanode.http.address: 0.0.0.0:9864\n","      hdfs:dfs.datanode.https.address: 0.0.0.0:9865\n","      hdfs:dfs.datanode.ipc.address: 0.0.0.0:9867\n","      hdfs:dfs.namenode.handler.count: '20'\n","      hdfs:dfs.namenode.http-address: 0.0.0.0:9870\n","      hdfs:dfs.namenode.https-address: 0.0.0.0:9871\n","      hdfs:dfs.namenode.lifeline.rpc-address: bdm-hw3-m:8050\n","      hdfs:dfs.namenode.secondary.http-address: 0.0.0.0:9868\n","      hdfs:dfs.namenode.secondary.https-address: 0.0.0.0:9869\n","      hdfs:dfs.namenode.service.handler.count: '10'\n","      hdfs:dfs.namenode.servicerpc-address: bdm-hw3-m:8051\n","      hive:hive.fetch.task.conversion: none\n","      mapred-env:HADOOP_JOB_HISTORYSERVER_HEAPSIZE: '3840'\n","      mapred:mapreduce.job.maps: '21'\n","      mapred:mapreduce.job.reduce.slowstart.completedmaps: '0.95'\n","      mapred:mapreduce.job.reduces: '7'\n","      mapred:mapreduce.jobhistory.recovery.store.class: org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService\n","      mapred:mapreduce.map.cpu.vcores: '1'\n","      mapred:mapreduce.map.java.opts: -Xmx2524m\n","      mapred:mapreduce.map.memory.mb: '3156'\n","      mapred:mapreduce.reduce.cpu.vcores: '1'\n","      mapred:mapreduce.reduce.java.opts: -Xmx2524m\n","      mapred:mapreduce.reduce.memory.mb: '3156'\n","      mapred:mapreduce.task.io.sort.mb: '256'\n","      mapred:yarn.app.mapreduce.am.command-opts: -Xmx2524m\n","      mapred:yarn.app.mapreduce.am.resource.cpu-vcores: '1'\n","      mapred:yarn.app.mapreduce.am.resource.mb: '3156'\n","      spark-env:SPARK_DAEMON_MEMORY: 3840m\n","      spark:spark.driver.maxResultSize: 1920m\n","      spark:spark.driver.memory: 3840m\n","      spark:spark.executor.cores: '2'\n","      spark:spark.executor.instances: '2'\n","      spark:spark.executor.memory: 5739m\n","      spark:spark.executorEnv.OPENBLAS_NUM_THREADS: '1'\n","      spark:spark.extraListeners: com.google.cloud.spark.performance.DataprocMetricsListener\n","      spark:spark.scheduler.mode: FAIR\n","      spark:spark.sql.cbo.enabled: 'true'\n","      spark:spark.ui.port: '0'\n","      spark:spark.yarn.am.memory: 640m\n","      yarn-env:YARN_NODEMANAGER_HEAPSIZE: '1536'\n","      yarn-env:YARN_RESOURCEMANAGER_HEAPSIZE: '3840'\n","      yarn-env:YARN_TIMELINESERVER_HEAPSIZE: '3840'\n","      yarn:yarn.nodemanager.address: 0.0.0.0:8026\n","      yarn:yarn.nodemanager.resource.cpu-vcores: '4'\n","      yarn:yarn.nodemanager.resource.memory-mb: '12624'\n","      yarn:yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: '86400'\n","      yarn:yarn.scheduler.maximum-allocation-mb: '12624'\n","      yarn:yarn.scheduler.minimum-allocation-mb: '1'\n","  tempBucket: dataproc-temp-us-west1-473379980650-ybburydq\n","  workerConfig:\n","    diskConfig:\n","      bootDiskSizeGb: 500\n","      bootDiskType: pd-standard\n","    imageUri: https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-2-0-deb10-20230415-165100-rc01\n","    instanceNames:\n","    - bdm-hw3-w-0\n","    - bdm-hw3-w-1\n","    machineTypeUri: https://www.googleapis.com/compute/v1/projects/bdma-384100/zones/us-west1-a/machineTypes/n1-standard-4\n","    minCpuPlatform: AUTOMATIC\n","    numInstances: 2\n","    preemptibility: NON_PREEMPTIBLE\n","labels:\n","  goog-dataproc-cluster-name: bdm-hw3\n","  goog-dataproc-cluster-uuid: 6479032e-cccd-4572-82a1-8a7c6a1ba9a8\n","  goog-dataproc-location: us-west1\n","projectId: bdma-384100\n","status:\n","  state: RUNNING\n","  stateStartTime: '2023-04-19T19:27:54.722437Z'\n","statusHistory:\n","- state: CREATING\n","  stateStartTime: '2023-04-19T19:26:06.514796Z'\n"]}],"source":["!gcloud dataproc clusters describe bdm-hw3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUZIzwyx8Ywd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1683091369487,"user":{"displayName":"Sagar Ghimire","userId":"11125001823516405745"},"user_tz":240},"id":"C7issLExBYkF","outputId":"34808a97-0c7e-4c61-b3ab-81127118a040"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing BDM_HW3_23735863_Ghimire.py\n"]}],"source":["%%writefile BDM_HW3_23735863_Ghimire.py\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","sc = pyspark.SparkContext.getOrCreate()\n","spark = SparkSession.builder.getOrCreate()\n","spark\n","## Importing all necessary libraries for calculation\n","from pyspark.sql.functions import year\n","from pyspark.sql.functions import lower, year, count, round\n","from pyspark.sql.functions import sum, expr\n","from pyspark.sql.types import StringType\n","from pyspark.sql.types import IntegerType\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import countDistinct\n","from pyspark.sql.functions import col, count, sum, ceil, max\n","from pyspark.sql.window import Window\n","import csv\n","##Dowloading the file fomr gc\n","\n","\n","\n","df=spark.read.csv(\"gs://bdma/data/complaints.csv\",header=True, multiLine=True, inferSchema=True, escape='\"')\n","## selecting only 4 columns for the analysis which are Date received(take only year), Product, Company and Complaint ID\n","dyear=df.select(\"Product\",(year(\"Date received\")).alias(\"year\"),\"Company\",\"Complaint ID\").sort(\"Product\",\"year\")\n","## Lowercasing the Product and Company columns values for further grouping and other calculation\n","ds_product = dyear.withColumn(\"Product\", lower(dyear[\"Product\"].cast(StringType())))\n","ds_company = ds_product.withColumn(\"Company\", lower(ds_product[\"Company\"].cast(StringType())))\n","##Grouping the 'ds_company' dataframe  by product and year\n","##counting the number of Compplaint ID  and number of distinct Company for total complaints and Companies count\n","ds_complaint=ds_company.groupBy(\"Product\", \"year\").agg(countDistinct(\"Complaint ID\").alias(\"total_complaints\"),countDistinct(\"Company\").alias(\"Companies Count\")).sort(\"Product\",\"year\")\n","\n","# Group by product, year, and company, and count Complaint ID for atleast one  complain of the company\n","## The grouped new data frame is \"grouped_df\"\n","grouped_df = ds_company.groupBy(\"Product\", \"year\", \"Company\").agg(count(\"Complaint ID\").alias(\"atleast_complaint_count\"))\n","## Joining two data frames by Product and year\n","grouped_df1=ds_complaint.join(grouped_df, [\"Product\", \"year\"])\n","\n","# Calculate the percentage of total complaints filed against one company for each product and year by dividing with 'total_complaints'\n","grouped_df2 = grouped_df1.withColumn((\"percentage\"), round(grouped_df1[\"atleast_complaint_count\"] / grouped_df1[\"total_complaints\"] * 100, 0).cast(IntegerType())).sort(\"Product\",\"year\")\n","## Joining the 'ds_complaint' dataframe and \"grouped_df2\" dataframe to bring all the above\n","##calculation together like company count, total complaint, atleast_complaint_count and percentage\n","dp=ds_complaint.join(grouped_df2, [\"Product\", \"year\",\"total_complaints\", \"Companies Count\"], \"left\").sort(\"Product\",\"year\")\n","## Grouping the dataframe 'dp' by product and year and printing only highest percentage\n","## of total complaints filed against one company for that product and year\n","gr = dp.groupBy(\"Product\", \"year\") .agg(max(\"percentage\").alias(\"highest_percentage\")).sort(\"Product\", \"year\")\n","## merging dataframe 'ds' and 'gr' to put 'total complaints', 'companies count' and 'highest percentage of complaint' together\n","final=ds_complaint.join(gr,[\"Product\", \"year\"]).sort(\"Product\",\"year\")\n","final.show(20)\n","\n","final.write.csv(\"gs://bdma/shared/2023_spring/HW3/23735863_Ghimire\")\n","## Converting the csv into spark rdd\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxmaKHKQCBjs","outputId":"32121b51-58b7-4628-f2c9-771df26ca96c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Job [a3d4d6b674ef4b449d18cc4ba93ebbbc] submitted.\n","Waiting for job output...\n","23/04/19 19:48:39 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","23/04/19 19:48:39 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","23/04/19 19:48:39 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","23/04/19 19:48:39 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","23/04/19 19:48:39 INFO org.sparkproject.jetty.util.log: Logging initialized @3269ms to org.sparkproject.jetty.util.log.Slf4jLog\n","23/04/19 19:48:39 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_362-b09\n","23/04/19 19:48:39 INFO org.sparkproject.jetty.server.Server: Started @3377ms\n","23/04/19 19:48:39 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@501f5554{HTTP/1.1, (http/1.1)}{0.0.0.0:36215}\n","23/04/19 19:48:40 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at bdm-hw3-m/10.138.0.17:8032\n","23/04/19 19:48:40 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at bdm-hw3-m/10.138.0.17:10200\n","23/04/19 19:48:42 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n","23/04/19 19:48:42 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","23/04/19 19:48:42 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1681932433534_0003\n","23/04/19 19:48:43 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at bdm-hw3-m/10.138.0.17:8030\n","23/04/19 19:48:45 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n","23/04/19 19:48:48 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1\n","23/04/19 19:48:48 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1\n","+--------------------+----+----------------+---------------+------------------+\n","|             Product|year|total_complaints|Companies Count|highest_percentage|\n","+--------------------+----+----------------+---------------+------------------+\n","|bank account or s...|2012|           12212|             98|                19|\n","|bank account or s...|2013|           13388|            164|                18|\n","|bank account or s...|2014|           14662|            258|                17|\n","|bank account or s...|2015|           17140|            215|                17|\n","|bank account or s...|2016|           21847|            231|                15|\n","|bank account or s...|2017|            6955|            173|                16|\n","|checking or savin...|2017|           12763|            183|                17|\n","|checking or savin...|2018|           21211|            214|                16|\n","|checking or savin...|2019|           21735|            249|                15|\n","|checking or savin...|2020|           24238|            268|                14|\n","|checking or savin...|2021|           29555|            289|                14|\n","|checking or savin...|2022|           37586|            346|                14|\n","|checking or savin...|2023|           12279|            224|                37|\n","|       consumer loan|2012|            1986|             84|                19|\n","|       consumer loan|2013|            3117|            159|                12|\n","|       consumer loan|2014|            5456|            357|                 8|\n","|       consumer loan|2015|            7882|            596|                 9|\n","|       consumer loan|2016|            9591|            664|                 7|\n","|       consumer loan|2017|            3544|            424|                 8|\n","|         credit card|2011|            1260|             33|                19|\n","+--------------------+----+----------------+---------------+------------------+\n","only showing top 20 rows\n","\n","23/04/19 19:50:20 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://bdma/shared/2023_spring/HW3/23735863_Ghimire/' directory.\n","23/04/19 19:50:20 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@501f5554{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n","Job [a3d4d6b674ef4b449d18cc4ba93ebbbc] finished successfully.\n","done: true\n","driverControlFilesUri: gs://dataproc-staging-us-west1-473379980650-f1bjrajq/google-cloud-dataproc-metainfo/6479032e-cccd-4572-82a1-8a7c6a1ba9a8/jobs/a3d4d6b674ef4b449d18cc4ba93ebbbc/\n","driverOutputResourceUri: gs://dataproc-staging-us-west1-473379980650-f1bjrajq/google-cloud-dataproc-metainfo/6479032e-cccd-4572-82a1-8a7c6a1ba9a8/jobs/a3d4d6b674ef4b449d18cc4ba93ebbbc/driveroutput\n","jobUuid: d6cba6e3-f78d-3a7e-bba8-d33cd187e64b\n","placement:\n","  clusterName: bdm-hw3\n","  clusterUuid: 6479032e-cccd-4572-82a1-8a7c6a1ba9a8\n","pysparkJob:\n","  args:\n","  - gs://bdma/data/complaints.csv\n","  - gs://bdma/shared/2023_spring/HW3/23735863_Ghimire\n","  mainPythonFileUri: gs://dataproc-staging-us-west1-473379980650-f1bjrajq/google-cloud-dataproc-metainfo/6479032e-cccd-4572-82a1-8a7c6a1ba9a8/jobs/a3d4d6b674ef4b449d18cc4ba93ebbbc/staging/BDM_HW3_23735863_Ghimire.py\n","reference:\n","  jobId: a3d4d6b674ef4b449d18cc4ba93ebbbc\n","  projectId: bdma-384100\n","status:\n","  state: DONE\n","  stateStartTime: '2023-04-19T19:50:24.548763Z'\n","statusHistory:\n","- state: PENDING\n","  stateStartTime: '2023-04-19T19:48:34.762293Z'\n","- state: SETUP_DONE\n","  stateStartTime: '2023-04-19T19:48:34.795185Z'\n","- details: Agent reported job success\n","  state: RUNNING\n","  stateStartTime: '2023-04-19T19:48:34.994341Z'\n","yarnApplications:\n","- name: BDM_HW3_23735863_Ghimire.py\n","  progress: 1.0\n","  state: FINISHED\n","  trackingUrl: http://bdm-hw3-m:8088/proxy/application_1681932433534_0003/\n"]}],"source":["!gcloud dataproc jobs submit pyspark --cluster bdm-hw3 BDM_HW3_23735863_Ghimire.py -- gs://bdma/data/complaints.csv gs://bdma/shared/2023_spring/HW3/23735863_Ghimire\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkiO4VCFs--p","outputId":"2bda8e1b-04c5-4ca7-8d47-681f965d2ea3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file:///content/report2.csv/part-00000-6b1375fd-488c-4eed-9499-82f18a318372-c000.csv [Content-Type=text/csv]...\n","/ [0 files][    0.0 B/  5.0 KiB]                                                \r/ [1 files][  5.0 KiB/  5.0 KiB]                                                \r\n","Operation completed over 1 objects/5.0 KiB.                                      \n"]}],"source":["##NOTE:- If running from begining\n","## Once the above ouput is printed look for report2. csv in collab and inside that look for a csv file \"part.......\" replace the current csv file path\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1014,"status":"ok","timestamp":1683092802114,"user":{"displayName":"Sagar Ghimire","userId":"11125001823516405745"},"user_tz":240},"id":"V8atujexgZS4","outputId":"18f09e38-c3e2-478d-802e-bbfaefedaf1a"},"outputs":[],"source":["!gsutil ls  gs://bdma/shared/2023_spring/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0c98t7CYbtXE"},"outputs":[],"source":["                        #####################################################################THE END#############################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9tnrBakEmg9"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
